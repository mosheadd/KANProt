{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea89deb",
   "metadata": {},
   "source": [
    "# Importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25ab13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from kan import KAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94695de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda_if_available = False\n",
    "device_name = \"cuda\" if torch.cuda.is_available() and use_cuda_if_available else \"cpu\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() and use_cuda_if_available else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a402a",
   "metadata": {},
   "source": [
    "# Load and preprocess images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "256e639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return ValueError(f\"Unable to load file {image_path}\")\n",
    "    \n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    img = img / 255\n",
    "\n",
    "    img_flattened = img.flatten()\n",
    "\n",
    "    return img_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b3095",
   "metadata": {},
   "source": [
    "# Load train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "936b9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_dataset(csv_path, images_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    print(\"Loading train images...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        img_filename = f\"{row['id_code']}.jpg\"\n",
    "        img_path = os.path.join(images_dir, img_filename)\n",
    "\n",
    "        try:\n",
    "            img_vector = load_and_preprocess_image(img_path)\n",
    "            x.append(img_vector)\n",
    "            y.append(row['diagnosis'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing image {img_path}: {e}\")\n",
    "\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Loaded {len(x)} size of train data\")\n",
    "    \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f1567",
   "metadata": {},
   "source": [
    "# Prepare data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ed108",
   "metadata": {},
   "source": [
    "### Get train data and split it for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74b570a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train images...\n",
      "Loaded 3662 size of train data\n",
      "Train and val data shape:\n",
      "Train: (3112, 12288), (3112,)\n",
      "Validation: (550, 12288), (550,)\n"
     ]
    }
   ],
   "source": [
    "trainCsvPath = \"data/trainLabels.csv\"\n",
    "trainImagesPath = \"data/train\"\n",
    "testCsvPath = \"data/testImages.csv\"\n",
    "testImagesPath = \"data/test\"\n",
    "\n",
    "x, y = load_train_dataset(trainCsvPath, trainImagesPath)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train and val data shape:\")\n",
    "print(f\"Train: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation: {x_val.shape}, {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919bc4c",
   "metadata": {},
   "source": [
    "### Compute class weights to solve class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c6bb2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed class weights:\n",
      "  Class 0: weight = 0.41\n",
      "  Class 1: weight = 1.98\n",
      "  Class 2: weight = 0.73\n",
      "  Class 3: weight = 3.80\n",
      "  Class 4: weight = 2.48\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Computed class weights:\")\n",
    "for cls, weight in class_weight_dict.items():\n",
    "    print(f\"  Class {cls}: weight = {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f100e6d",
   "metadata": {},
   "source": [
    "### Transform into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c0b6632",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "if use_cuda_if_available:\n",
    "    x_train_tensor = x_train_tensor.to(DEVICE)\n",
    "    y_train_tensor = y_train_tensor.to(DEVICE)\n",
    "    x_val_tensor = x_val_tensor.to(DEVICE)\n",
    "    y_val_tensor = y_val_tensor.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d98b7f",
   "metadata": {},
   "source": [
    "# Create and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b79277d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "MultKAN(\n",
      "  (act_fun): ModuleList(\n",
      "    (0-1): 2 x KANLayer(\n",
      "      (base_fun): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (base_fun): SiLU()\n",
      "  (symbolic_fun): ModuleList(\n",
      "    (0-1): 2 x Symbolic_KANLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(100)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = KAN(width=[input_dim, 32, output_dim], grid=3, k=2, seed=42, device=device_name)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569675e",
   "metadata": {},
   "source": [
    "# Training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_val(model, x_train, y_train, x_val, y_val, epochs=9999, lr=1e-3, print_epoch_every=5, early_stopping=True, class_weight_dict=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    if class_weight_dict:\n",
    "        weight_tensor = torch.tensor(list(class_weight_dict.values()), dtype=torch.float32).to(DEVICE)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "\n",
    "    last_val_loss = 999999.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        predictions = model(x_train)\n",
    "\n",
    "        if class_weight_dict:\n",
    "            loss = criterion(predictions, y_train)\n",
    "        else:\n",
    "            loss = torch.nn.functional.cross_entropy(predictions, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(x_val)\n",
    "            val_loss =  torch.nn.functional.cross_entropy(val_predictions, y_val)\n",
    "            val_pred_classes = torch.argmax(val_predictions, dim=1)\n",
    "            val_accuracy = accuracy_score(y_val.numpy(), val_pred_classes.numpy())\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "        val_acc.append(val_accuracy)\n",
    "\n",
    "        if epoch % print_epoch_every == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch: {epoch}; train loss={loss.item():.4f}; val loss: {val_loss.item():.4f}; val acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if (val_loss.item() >= last_val_loss) and early_stopping:\n",
    "            break\n",
    "\n",
    "        last_val_loss = val_loss.item()\n",
    "\n",
    "    return train_losses, val_losses, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "122a837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch: 0; train loss=1.5990; val loss: 1.5527; val acc: 0.6691\n",
      "Epoch: 5; train loss=1.5815; val loss: 1.5599; val acc: 0.5836\n",
      "Epoch: 10; train loss=1.5625; val loss: 1.5368; val acc: 0.5636\n",
      "Epoch: 15; train loss=1.5396; val loss: 1.4948; val acc: 0.6091\n",
      "Epoch: 20; train loss=1.5129; val loss: 1.4424; val acc: 0.6345\n",
      "Epoch: 25; train loss=1.4832; val loss: 1.3844; val acc: 0.6364\n",
      "Epoch: 30; train loss=1.4519; val loss: 1.3246; val acc: 0.6291\n",
      "Epoch: 35; train loss=1.4209; val loss: 1.2680; val acc: 0.6055\n",
      "Epoch: 40; train loss=1.3923; val loss: 1.2189; val acc: 0.5855\n",
      "Epoch: 45; train loss=1.3678; val loss: 1.1795; val acc: 0.5691\n",
      "Epoch: 50; train loss=1.3488; val loss: 1.1506; val acc: 0.5600\n",
      "Epoch: 55; train loss=1.3352; val loss: 1.1325; val acc: 0.5582\n",
      "Epoch: 60; train loss=1.3259; val loss: 1.1230; val acc: 0.5545\n",
      "Epoch: 65; train loss=1.3194; val loss: 1.1177; val acc: 0.5491\n",
      "Epoch: 70; train loss=1.3143; val loss: 1.1134; val acc: 0.5400\n",
      "Epoch: 75; train loss=1.3096; val loss: 1.1094; val acc: 0.5291\n",
      "Epoch: 80; train loss=1.3050; val loss: 1.1055; val acc: 0.5327\n",
      "Epoch: 85; train loss=1.3006; val loss: 1.1011; val acc: 0.5255\n",
      "Epoch: 90; train loss=1.2963; val loss: 1.0966; val acc: 0.5218\n",
      "Epoch: 95; train loss=1.2922; val loss: 1.0924; val acc: 0.5255\n",
      "Epoch: 100; train loss=1.2882; val loss: 1.0883; val acc: 0.5345\n",
      "Epoch: 105; train loss=1.2844; val loss: 1.0843; val acc: 0.5418\n",
      "Epoch: 110; train loss=1.2807; val loss: 1.0807; val acc: 0.5436\n",
      "Epoch: 115; train loss=1.2772; val loss: 1.0771; val acc: 0.5436\n",
      "Epoch: 120; train loss=1.2739; val loss: 1.0741; val acc: 0.5436\n",
      "Epoch: 125; train loss=1.2707; val loss: 1.0715; val acc: 0.5436\n",
      "Epoch: 130; train loss=1.2677; val loss: 1.0693; val acc: 0.5455\n",
      "Epoch: 135; train loss=1.2648; val loss: 1.0675; val acc: 0.5509\n",
      "Epoch: 140; train loss=1.2620; val loss: 1.0656; val acc: 0.5600\n",
      "Epoch: 145; train loss=1.2593; val loss: 1.0637; val acc: 0.5655\n",
      "Epoch: 150; train loss=1.2566; val loss: 1.0619; val acc: 0.5636\n",
      "Epoch: 155; train loss=1.2539; val loss: 1.0600; val acc: 0.5673\n",
      "Epoch: 160; train loss=1.2512; val loss: 1.0583; val acc: 0.5655\n",
      "Epoch: 165; train loss=1.2484; val loss: 1.0567; val acc: 0.5673\n",
      "Epoch: 170; train loss=1.2456; val loss: 1.0552; val acc: 0.5655\n",
      "Epoch: 175; train loss=1.2427; val loss: 1.0536; val acc: 0.5673\n",
      "Epoch: 180; train loss=1.2398; val loss: 1.0521; val acc: 0.5655\n",
      "Epoch: 185; train loss=1.2368; val loss: 1.0505; val acc: 0.5673\n",
      "Epoch: 190; train loss=1.2337; val loss: 1.0488; val acc: 0.5673\n",
      "Epoch: 195; train loss=1.2306; val loss: 1.0471; val acc: 0.5673\n",
      "Epoch: 200; train loss=1.2274; val loss: 1.0453; val acc: 0.5673\n",
      "Epoch: 205; train loss=1.2241; val loss: 1.0435; val acc: 0.5709\n",
      "Epoch: 210; train loss=1.2208; val loss: 1.0417; val acc: 0.5709\n",
      "Epoch: 215; train loss=1.2175; val loss: 1.0399; val acc: 0.5691\n",
      "Epoch: 220; train loss=1.2142; val loss: 1.0382; val acc: 0.5727\n",
      "Epoch: 225; train loss=1.2109; val loss: 1.0365; val acc: 0.5745\n",
      "Epoch: 230; train loss=1.2076; val loss: 1.0348; val acc: 0.5745\n",
      "Epoch: 235; train loss=1.2044; val loss: 1.0329; val acc: 0.5818\n",
      "Epoch: 240; train loss=1.2012; val loss: 1.0313; val acc: 0.5800\n",
      "Epoch: 245; train loss=1.1980; val loss: 1.0287; val acc: 0.5764\n",
      "Epoch: 250; train loss=1.1950; val loss: 1.0278; val acc: 0.5745\n",
      "Epoch: 255; train loss=1.1920; val loss: 1.0277; val acc: 0.5782\n",
      "Epoch: 260; train loss=1.1891; val loss: 1.0247; val acc: 0.5764\n",
      "Epoch: 265; train loss=1.1862; val loss: 1.0256; val acc: 0.5800\n",
      "Epoch: 270; train loss=1.1834; val loss: 1.0233; val acc: 0.5891\n",
      "Epoch: 275; train loss=1.1805; val loss: 1.0216; val acc: 0.5909\n",
      "Epoch: 280; train loss=1.1776; val loss: 1.0218; val acc: 0.5891\n",
      "Epoch: 285; train loss=1.1747; val loss: 1.0207; val acc: 0.5909\n",
      "Epoch: 290; train loss=1.1717; val loss: 1.0193; val acc: 0.5909\n",
      "Epoch: 295; train loss=1.1686; val loss: 1.0181; val acc: 0.5945\n",
      "Epoch: 300; train loss=1.1656; val loss: 1.0174; val acc: 0.5945\n",
      "Epoch: 305; train loss=1.1624; val loss: 1.0162; val acc: 0.5927\n",
      "Epoch: 310; train loss=1.1594; val loss: 1.0157; val acc: 0.5964\n",
      "Epoch: 315; train loss=1.1562; val loss: 1.0149; val acc: 0.5909\n",
      "Epoch: 320; train loss=1.1530; val loss: 1.0142; val acc: 0.5927\n",
      "Epoch: 325; train loss=1.1498; val loss: 1.0130; val acc: 0.5927\n",
      "Epoch: 330; train loss=1.1465; val loss: 1.0116; val acc: 0.5945\n",
      "Epoch: 335; train loss=1.1431; val loss: 1.0104; val acc: 0.5909\n",
      "Epoch: 340; train loss=1.1396; val loss: 1.0094; val acc: 0.5945\n",
      "Epoch: 345; train loss=1.1361; val loss: 1.0082; val acc: 0.5927\n",
      "Epoch: 350; train loss=1.1324; val loss: 1.0067; val acc: 0.5927\n",
      "Epoch: 355; train loss=1.1287; val loss: 1.0059; val acc: 0.5873\n",
      "Epoch: 360; train loss=1.1249; val loss: 1.0064; val acc: 0.5891\n",
      "Epoch: 365; train loss=1.1210; val loss: 1.0049; val acc: 0.5891\n",
      "Epoch: 370; train loss=1.1170; val loss: 1.0032; val acc: 0.5891\n",
      "Epoch: 375; train loss=1.1130; val loss: 1.0012; val acc: 0.5909\n",
      "Epoch: 380; train loss=1.1088; val loss: 1.0003; val acc: 0.5927\n",
      "Epoch: 385; train loss=1.1046; val loss: 0.9982; val acc: 0.5927\n",
      "Epoch: 390; train loss=1.1002; val loss: 0.9966; val acc: 0.5982\n",
      "Epoch: 395; train loss=1.0959; val loss: 0.9905; val acc: 0.6036\n",
      "Epoch: 400; train loss=1.0916; val loss: 0.9892; val acc: 0.6091\n",
      "Epoch: 405; train loss=1.0871; val loss: 0.9896; val acc: 0.6127\n",
      "Epoch: 410; train loss=1.0826; val loss: 0.9934; val acc: 0.5982\n",
      "Epoch: 415; train loss=1.0780; val loss: 0.9916; val acc: 0.6000\n",
      "Epoch: 420; train loss=1.0734; val loss: 0.9882; val acc: 0.6073\n",
      "Epoch: 425; train loss=1.0688; val loss: 0.9888; val acc: 0.6000\n",
      "Epoch: 430; train loss=1.0640; val loss: 0.9882; val acc: 0.6000\n",
      "Epoch: 435; train loss=1.0591; val loss: 0.9857; val acc: 0.6055\n",
      "Epoch: 440; train loss=1.0543; val loss: 0.9842; val acc: 0.6073\n",
      "Epoch: 445; train loss=1.0492; val loss: 0.9842; val acc: 0.6055\n",
      "Epoch: 450; train loss=1.0443; val loss: 0.9846; val acc: 0.6055\n",
      "Epoch: 455; train loss=1.0394; val loss: 0.9811; val acc: 0.6055\n",
      "Epoch: 460; train loss=1.0344; val loss: 0.9816; val acc: 0.6073\n",
      "Epoch: 465; train loss=1.0294; val loss: 0.9794; val acc: 0.6145\n",
      "Epoch: 470; train loss=1.0242; val loss: 0.9786; val acc: 0.6091\n",
      "Epoch: 475; train loss=1.0191; val loss: 0.9787; val acc: 0.6091\n",
      "Epoch: 480; train loss=1.0139; val loss: 0.9774; val acc: 0.6164\n",
      "Epoch: 485; train loss=1.0086; val loss: 0.9774; val acc: 0.6164\n",
      "Epoch: 490; train loss=1.0035; val loss: 0.9783; val acc: 0.6036\n",
      "Epoch: 495; train loss=0.9981; val loss: 0.9772; val acc: 0.6091\n",
      "Epoch: 500; train loss=0.9929; val loss: 0.9768; val acc: 0.6145\n",
      "Epoch: 505; train loss=0.9876; val loss: 0.9763; val acc: 0.6164\n",
      "Epoch: 510; train loss=0.9823; val loss: 0.9771; val acc: 0.6145\n",
      "Epoch: 515; train loss=0.9773; val loss: 0.9751; val acc: 0.6218\n",
      "Epoch: 520; train loss=0.9718; val loss: 0.9753; val acc: 0.6236\n",
      "Epoch: 525; train loss=0.9664; val loss: 0.9762; val acc: 0.6200\n",
      "Epoch: 530; train loss=0.9610; val loss: 0.9769; val acc: 0.6200\n",
      "Epoch: 535; train loss=0.9557; val loss: 0.9787; val acc: 0.6164\n",
      "Epoch: 540; train loss=0.9507; val loss: 0.9772; val acc: 0.6145\n",
      "Epoch: 545; train loss=0.9452; val loss: 0.9781; val acc: 0.6109\n",
      "Epoch: 550; train loss=0.9398; val loss: 0.9794; val acc: 0.6109\n",
      "Epoch: 555; train loss=0.9345; val loss: 0.9802; val acc: 0.6091\n",
      "Epoch: 560; train loss=0.9292; val loss: 0.9806; val acc: 0.6109\n",
      "Epoch: 565; train loss=0.9239; val loss: 0.9811; val acc: 0.6091\n",
      "Epoch: 570; train loss=0.9186; val loss: 0.9819; val acc: 0.6109\n",
      "Epoch: 575; train loss=0.9139; val loss: 0.9825; val acc: 0.6055\n",
      "Epoch: 580; train loss=0.9082; val loss: 0.9818; val acc: 0.6091\n",
      "Epoch: 585; train loss=0.9034; val loss: 0.9831; val acc: 0.6018\n",
      "Epoch: 590; train loss=0.8982; val loss: 0.9855; val acc: 0.6055\n",
      "Epoch: 595; train loss=0.8930; val loss: 0.9870; val acc: 0.6036\n",
      "Epoch: 600; train loss=0.8884; val loss: 0.9866; val acc: 0.6036\n",
      "Epoch: 605; train loss=0.8831; val loss: 0.9887; val acc: 0.6073\n",
      "Epoch: 610; train loss=0.8783; val loss: 0.9889; val acc: 0.6000\n",
      "Epoch: 615; train loss=0.8734; val loss: 0.9909; val acc: 0.5982\n",
      "Epoch: 620; train loss=0.8683; val loss: 0.9915; val acc: 0.5982\n",
      "Epoch: 625; train loss=0.8634; val loss: 0.9907; val acc: 0.6073\n",
      "Epoch: 630; train loss=0.8584; val loss: 0.9921; val acc: 0.6055\n",
      "Epoch: 635; train loss=0.8537; val loss: 0.9962; val acc: 0.6018\n",
      "Epoch: 640; train loss=0.8491; val loss: 0.9933; val acc: 0.6145\n",
      "Epoch: 645; train loss=0.8439; val loss: 0.9943; val acc: 0.6127\n",
      "Epoch: 650; train loss=0.8387; val loss: 0.9971; val acc: 0.6073\n",
      "Epoch: 655; train loss=0.8341; val loss: 0.9996; val acc: 0.6055\n",
      "Epoch: 660; train loss=0.8290; val loss: 0.9991; val acc: 0.6127\n",
      "Epoch: 665; train loss=0.8243; val loss: 1.0004; val acc: 0.6145\n",
      "Epoch: 670; train loss=0.8193; val loss: 1.0025; val acc: 0.6145\n",
      "Epoch: 675; train loss=0.8148; val loss: 0.9993; val acc: 0.6200\n",
      "Epoch: 680; train loss=0.8105; val loss: 1.0042; val acc: 0.6182\n",
      "Epoch: 685; train loss=0.8053; val loss: 1.0052; val acc: 0.6182\n",
      "Epoch: 690; train loss=0.8010; val loss: 1.0041; val acc: 0.6200\n",
      "Epoch: 695; train loss=0.7963; val loss: 1.0081; val acc: 0.6145\n",
      "Epoch: 700; train loss=0.7916; val loss: 1.0097; val acc: 0.6164\n",
      "Epoch: 705; train loss=0.7873; val loss: 1.0128; val acc: 0.6145\n",
      "Epoch: 710; train loss=0.7828; val loss: 1.0117; val acc: 0.6200\n",
      "Epoch: 715; train loss=0.7788; val loss: 1.0164; val acc: 0.6145\n",
      "Epoch: 720; train loss=0.7746; val loss: 1.0134; val acc: 0.6200\n",
      "Epoch: 725; train loss=0.7702; val loss: 1.0144; val acc: 0.6200\n",
      "Epoch: 730; train loss=0.7655; val loss: 1.0161; val acc: 0.6236\n",
      "Epoch: 735; train loss=0.7612; val loss: 1.0192; val acc: 0.6200\n",
      "Epoch: 740; train loss=0.7572; val loss: 1.0225; val acc: 0.6145\n",
      "Epoch: 745; train loss=0.7528; val loss: 1.0235; val acc: 0.6200\n",
      "Epoch: 750; train loss=0.7488; val loss: 1.0235; val acc: 0.6200\n",
      "Epoch: 755; train loss=0.7449; val loss: 1.0292; val acc: 0.6182\n",
      "Epoch: 760; train loss=0.7408; val loss: 1.0271; val acc: 0.6200\n",
      "Epoch: 765; train loss=0.7364; val loss: 1.0312; val acc: 0.6182\n",
      "Epoch: 770; train loss=0.7323; val loss: 1.0330; val acc: 0.6164\n",
      "Epoch: 775; train loss=0.7285; val loss: 1.0328; val acc: 0.6218\n",
      "Epoch: 780; train loss=0.7269; val loss: 1.0440; val acc: 0.6145\n",
      "Epoch: 785; train loss=0.7217; val loss: 1.0427; val acc: 0.6109\n",
      "Epoch: 790; train loss=0.7175; val loss: 1.0430; val acc: 0.6127\n",
      "Epoch: 795; train loss=0.7135; val loss: 1.0440; val acc: 0.6145\n",
      "Epoch: 800; train loss=0.7095; val loss: 1.0451; val acc: 0.6145\n",
      "Epoch: 805; train loss=0.7056; val loss: 1.0473; val acc: 0.6145\n",
      "Epoch: 810; train loss=0.7018; val loss: 1.0493; val acc: 0.6145\n",
      "Epoch: 815; train loss=0.6981; val loss: 1.0521; val acc: 0.6182\n",
      "Epoch: 820; train loss=0.6945; val loss: 1.0540; val acc: 0.6182\n",
      "Epoch: 825; train loss=0.6908; val loss: 1.0571; val acc: 0.6200\n",
      "Epoch: 830; train loss=0.6873; val loss: 1.0567; val acc: 0.6218\n",
      "Epoch: 835; train loss=0.6871; val loss: 1.0687; val acc: 0.6145\n",
      "Epoch: 840; train loss=0.6815; val loss: 1.0686; val acc: 0.6145\n",
      "Epoch: 845; train loss=0.6771; val loss: 1.0688; val acc: 0.6182\n",
      "Epoch: 850; train loss=0.6734; val loss: 1.0695; val acc: 0.6236\n",
      "Epoch: 855; train loss=0.6701; val loss: 1.0721; val acc: 0.6182\n",
      "Epoch: 860; train loss=0.6666; val loss: 1.0743; val acc: 0.6182\n",
      "Epoch: 865; train loss=0.6631; val loss: 1.0769; val acc: 0.6200\n",
      "Epoch: 870; train loss=0.6597; val loss: 1.0787; val acc: 0.6200\n",
      "Epoch: 875; train loss=0.6564; val loss: 1.0807; val acc: 0.6200\n",
      "Epoch: 880; train loss=0.6530; val loss: 1.0836; val acc: 0.6200\n",
      "Epoch: 885; train loss=0.6497; val loss: 1.0868; val acc: 0.6200\n",
      "Epoch: 890; train loss=0.6464; val loss: 1.0878; val acc: 0.6218\n",
      "Epoch: 895; train loss=0.6433; val loss: 1.0939; val acc: 0.6200\n",
      "Epoch: 900; train loss=0.6420; val loss: 1.0898; val acc: 0.6200\n",
      "Epoch: 905; train loss=0.6365; val loss: 1.0940; val acc: 0.6218\n",
      "Epoch: 910; train loss=0.6334; val loss: 1.0986; val acc: 0.6200\n",
      "Epoch: 915; train loss=0.6305; val loss: 1.1024; val acc: 0.6218\n",
      "Epoch: 920; train loss=0.6273; val loss: 1.1057; val acc: 0.6200\n",
      "Epoch: 925; train loss=0.6238; val loss: 1.1077; val acc: 0.6200\n",
      "Epoch: 930; train loss=0.6206; val loss: 1.1088; val acc: 0.6200\n",
      "Epoch: 935; train loss=0.6175; val loss: 1.1104; val acc: 0.6182\n",
      "Epoch: 940; train loss=0.6143; val loss: 1.1137; val acc: 0.6200\n",
      "Epoch: 945; train loss=0.6112; val loss: 1.1176; val acc: 0.6218\n",
      "Epoch: 950; train loss=0.6084; val loss: 1.1170; val acc: 0.6127\n",
      "Epoch: 955; train loss=0.6060; val loss: 1.1235; val acc: 0.6200\n",
      "Epoch: 960; train loss=0.6021; val loss: 1.1262; val acc: 0.6182\n",
      "Epoch: 965; train loss=0.5990; val loss: 1.1253; val acc: 0.6127\n",
      "Epoch: 970; train loss=0.5980; val loss: 1.1366; val acc: 0.6145\n",
      "Epoch: 975; train loss=0.5931; val loss: 1.1329; val acc: 0.6145\n",
      "Epoch: 980; train loss=0.5905; val loss: 1.1326; val acc: 0.6127\n",
      "Epoch: 985; train loss=0.5874; val loss: 1.1341; val acc: 0.6145\n",
      "Epoch: 990; train loss=0.5841; val loss: 1.1371; val acc: 0.6164\n",
      "Epoch: 995; train loss=0.5812; val loss: 1.1413; val acc: 0.6164\n",
      "Epoch: 1000; train loss=0.5784; val loss: 1.1443; val acc: 0.6164\n",
      "Epoch: 1005; train loss=0.5754; val loss: 1.1452; val acc: 0.6182\n",
      "Epoch: 1010; train loss=0.5725; val loss: 1.1475; val acc: 0.6200\n",
      "Epoch: 1015; train loss=0.5697; val loss: 1.1523; val acc: 0.6127\n",
      "Epoch: 1020; train loss=0.5672; val loss: 1.1511; val acc: 0.6164\n",
      "Epoch: 1025; train loss=0.5659; val loss: 1.1630; val acc: 0.6145\n",
      "Epoch: 1030; train loss=0.5611; val loss: 1.1600; val acc: 0.6164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_losses, val_losses, val_acc = \u001b[43mtrain_model_with_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m fig, axes = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m12\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m      6\u001b[39m axes[\u001b[32m0\u001b[39m].plot(train_losses, label=\u001b[33m'\u001b[39m\u001b[33mTrain Loss\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_model_with_val\u001b[39m\u001b[34m(model, x_train, y_train, x_val, y_val, epochs, lr, print_epoch_every, early_stopping, class_weight_dict)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m     14\u001b[39m     model.train()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m class_weight_dict:\n\u001b[32m     18\u001b[39m         loss = criterion(predictions, y_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\kan\\MultKAN.py:804\u001b[39m, in \u001b[36mMultKAN.forward\u001b[39m\u001b[34m(self, x, singularity_avoiding, y_th)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m#print(preacts, postacts_numerical, postspline)\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.symbolic_enabled == \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m804\u001b[39m     x_symbolic, postacts_symbolic = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msymbolic_fun\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingularity_avoiding\u001b[49m\u001b[43m=\u001b[49m\u001b[43msingularity_avoiding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_th\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_th\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    806\u001b[39m     x_symbolic = \u001b[32m0.\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\kan\\Symbolic_KANLayer.py:116\u001b[39m, in \u001b[36mSymbolic_KANLayer.forward\u001b[39m\u001b[34m(self, x, singularity_avoiding, y_th)\u001b[39m\n\u001b[32m    114\u001b[39m         xij = \u001b[38;5;28mself\u001b[39m.affine[j,i,\u001b[32m2\u001b[39m]*\u001b[38;5;28mself\u001b[39m.funs_avoid_singularity[j][i](\u001b[38;5;28mself\u001b[39m.affine[j,i,\u001b[32m0\u001b[39m]*x[:,[i]]+\u001b[38;5;28mself\u001b[39m.affine[j,i,\u001b[32m1\u001b[39m], torch.tensor(y_th))[\u001b[32m1\u001b[39m]+\u001b[38;5;28mself\u001b[39m.affine[j,i,\u001b[32m3\u001b[39m]\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         xij = \u001b[38;5;28mself\u001b[39m.affine[j,i,\u001b[32m2\u001b[39m]*\u001b[38;5;28mself\u001b[39m.funs[j][i](\u001b[38;5;28mself\u001b[39m.affine[j,i,\u001b[32m0\u001b[39m]*x[:,[i]]+\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maffine\u001b[49m[j,i,\u001b[32m1\u001b[39m])+\u001b[38;5;28mself\u001b[39m.affine[j,i,\u001b[32m3\u001b[39m]\n\u001b[32m    117\u001b[39m     postacts_.append(\u001b[38;5;28mself\u001b[39m.mask[j][i]*xij)\n\u001b[32m    118\u001b[39m postacts.append(torch.stack(postacts_))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1951\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1946\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1953\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "train_losses, val_losses, val_acc = train_model_with_val(model, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor, early_stopping=False, class_weight_dict=class_weight_dict)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train Loss')\n",
    "axes[0].plot(val_losses, label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss plot')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(val_acc)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy plot')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241691c",
   "metadata": {},
   "source": [
    "# Validate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40e92042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 61.82%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90       271\n",
      "           1       0.24      0.27      0.25        56\n",
      "           2       0.56      0.45      0.50       150\n",
      "           3       0.20      0.31      0.25        29\n",
      "           4       0.15      0.20      0.17        44\n",
      "\n",
      "    accuracy                           0.62       550\n",
      "   macro avg       0.41      0.42      0.41       550\n",
      "weighted avg       0.65      0.62      0.63       550\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAokAAAIjCAYAAABvUIGpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASzhJREFUeJzt3QdcVfX/x/EPoCIOMDemOHOVe++9c7bsb6lpmjN3RpYrFWfumblKS82VZpppaRqmYpqSmpql5gD1JwgKDvg/vt+CuH61QLncezmvZ4/zgHvuuZev53QvH97fcd1iY2NjBQAAAEjAPeENAAAAgCIRAAAAD0SSCAAAAANFIgAAAAwUiQAAADBQJAIAAMBAkQgAAAADRSIAAAAMFIkAAAAwUCQCeKiTJ09K48aNxcfHR9zc3GT9+vXJerZ+//13/bxLlizhKtynQIEC0rlzZ84LAIehSASc3OnTp+WNN96QQoUKSfr06cXb21tq1Kgh06dPl1u3btn1Z3fq1EmOHDkiY8eOlY8//lgqVqxo15+XGv3yyy8ycuRIXRADgCtx47ObAef15ZdfygsvvCCenp7SsWNHeeaZZ+T27duye/duWbNmjU6aFixYYJefrQrQDBkyyLBhw2TMmDF2+Rnqo+Ojo6Mlbdq04uHhIanR559/rq/ht99+K3Xr1k3049R5cXd31+cGABwhjUN+KoD/dObMGWnfvr3kz59fduzYIb6+vvH39e7dW06dOqWLSHsJDQ3VX7NkyWK3n6G6mlU6in+K5qioKPHy8tJ/GACAI9HdDDipiRMnSkREhHz00Uc2BWKcIkWKSL9+/eJv3717V95//30pXLiwLjDUmLZ33nlHJ1IJqf3PPvusTiMrV66sizTVlb1s2bL4Y1T3qCpOlSFDhuhiTj1OUell3PcJqceo4xLatm2b1KxZUxeamTJlkmLFiuk2/deYRFUU16pVSzJmzKgf27p1azl27NgDf54qllWb1HFq7ORrr70mN2/e/M/zq1I9lcz+/PPPUqdOHZ2aqnOqkj9l586dUqVKFV2wqXZ/8803No//448/pFevXvo+dUy2bNl0YpiwW1n9u9Q+pV69erq9avvuu+9srsXWrVt1V756nvnz5xtjElXxqB6fI0cOCQkJiX9+lSqXKlVKX/PIyMj//DcDQFJQJAJOauPGjbp4q169eqKOf/3112X48OFSvnx5mTp1qi58AgICdBp5P1VYPf/889KoUSOZMmWKPPHEE7ogCQ4O1ve3a9dOP4fy8ssv6/GI06ZNS1L71XOpAkgVqaNHj9Y/p1WrVrJnz55/fZwqxpo0aaKLIVUIDhw4UH744Qc9DvNB4/pefPFFuXHjhv63qu9VYTZq1KhEtfF///ufbqMqBlVRroprdb5WrlypvzZv3lzGjx+vCzB1vtTPibN//37dLnXcjBkzpEePHrJ9+3ZdfMYVqbVr15Y333xTf6+KY3Ue1VaiRIn45zlx4oQ+x+paqHGmZcuWNdqpCstFixbplFH9nDgjRozQ53nx4sW6oAaAZKXGJAJwLmFhYbHq5dm6detEHX/o0CF9/Ouvv26zf/DgwXr/jh074vflz59f79u1a1f8vpCQkFhPT8/YQYMGxe87c+aMPm7SpEk2z9mpUyf9HPcbMWKEPj7O1KlT9e3Q0NCHtjvuZyxevDh+X9myZWNz5swZe/Xq1fh9hw8fjnV3d4/t2LGj8fO6dOli85xt27aNzZYtW+x/qVOnjn78ihUr4vcdP35c71M/a+/evfH7t27darTz5s2bxnMGBgbq45YtWxa/b/Xq1Xrft99+axwfdy22bNnywPvUuU5o/vz5+vhPPvlEt8/DwyO2f//+//lvBYBHQZIIOKHw8HD9NXPmzIk6fvPmzfqrSt0SGjRokP56/9jFkiVL6u7cOKobU3Wb/vbbb5Jc4sYybtiwQWJiYhL1mIsXL8qhQ4d0qpk1a9b4/aVLl9ZJW9y/M6GEyZqi/l1Xr16NP4f/RnWBJ0xa1TlQ7VZJn0oX48R9n/D8qK7hOHfu3NE/U3VXq8cfPHhQEqtgwYI6OU2M7t2762P79u0rr776qu5mHjduXKJ/FgAkBUUi4ITUMjdKwu7Nf6PGx6mZsKpISSh37ty6aFH3J+Tn52c8h+pyVt2vyeWll17SXcSqGzxXrly6GFu1atW/Foxx7VTF2v1U4XblyhVj7N39/xb171AS82/JmzevMY5SjWvMly+fse/+51Szv1X3vjpWdVNnz55dF9vXr1+XsLAwSUqRmBRqjKrqzlZrWKqu9YTFKgAkJ4pEwEmLxDx58sjRo0eT9Lj7C56HedhyM2qCxKP+jHv37tncVsXLrl279BhDlXqpCSKqcFSJ4P3HPo7H+bc87LGJeU6V5qn1I9U4SFX8fv3113qijprAktjkVElqkacmvcRNRlJrWAKAvVAkAk5KTahQC2kHBgb+57FqJrIqTFS6lNDly5d1shU3Uzk5qKROPef97k8rFZVuNmjQQD744AO9qLQqqtTMZbVm4MP+HXGTOe53/PhxndY5ywQNNQtaLTauJuTETQJSM7nvPzeJLdwT2x2vilP1KTjq/4/Bgwc/8LwDQHKgSASc1FtvvaULItVdq4q9+6kCUs2GVdQsXOX+GciqOFNatGiRbO1S4+BUd6pKBhMWL+vWrbM57tq1a8Zj42bu3r8sTxy11I86ZunSpTbFlkpUVVIX9+90BiptvD+tnDlzppGSxhW1Dyqsk6pbt276jwHV5awWUU+TJo107do1UakpACQVi2kDTkoVYytWrNBdtGo8XsJPXFFLr6xevTp+Hb0yZcroVEsVDqoYUcvf7Nu3Txdbbdq00WvsJRc1tnDo0KHStm1bvbyLGh83d+5cKVq0qM2EDbXsjepuVgWqSgjVkjZz5szR4wBV4vYwkyZNkmbNmkm1atV0AaTG/qniS40LVEviOAuV5KnlbFS71EQglfiqrnXV3ZyQKnpVQTlhwgRdXKvxi/Xr15ecOXMm6eepZW7UBCQ1DlGdQ0Wdl1deeUWff7VmIwAkJ4pEwImpdQVVYqcKJzVLWBUDqshQs31VN6dKluIsXLhQr6uoigiV6qlJK/7+/notveSkiiD1/GomtUo71cQLtUah6upOWCSqtqt1DdX6fmrCieoqVsWrWsMwbiLIgzRs2FC2bNmi260mhqiPpVOPU0VWUid52JNKcVXxt3z5cr1+oZqkE7fGY0LqOsybN0+fI1X0qqRRdbcnpUg8f/68DBgwQFq2bKn/GIjToUMH/fGM6jqowtqZzg8A18dnNwMAAMDAmEQAAAAYKBIBAABgoEgEAACAgSIRAAAABopEAAAAGCgSAQAAYKBIBAAAgDUW0/Yq18fRTcDfQvfO5Fw4kXsxfHybs/BwT77PdMbjieFjDZ2Gd3r3VFk73PpplrgikkQAAABYI0kEAABIEjdys/tRJAIAALgxBOR+lM0AAAAwkCQCAADQ3WwgSQQAAICBJBEAAIAxiQaSRAAAABhIEgEAABiTaCBJBAAAgIEkEQAAgDGJBopEAAAAupsNdDcDAADAQJIIAABAd7OBJBEAAAAGkkQAAADGJBpIEgEAAGAgSQQAAGBMooEkEQAAAAaSRAAAAMYkGigSAQAA6G420N0MAAAAA0kiAAAA3c0GkkQAAAAYSBIBAABIEg0kiQAAADCQJAIAALi7cQ7uQ5IIAAAAA0kiAAAAYxINFIkAAAAspm2guxkAAAAGkkQAAAC6mw0kiQAAADCQJAIAADAm0UCSCAAAAANJIgAAAGMSDSSJAAAAMJAkAgAAMCbRQJEIAABAd7OB7mYAAAAYKBJT0OAujWX3J0MkZPdk+WN7gKz6oJs8lT/nQ49fP6un3PpplrSsW9pmf77cT8jaGT3k6g8f6OcZ17+NeHhwKe0h5PJledd/iNSvVUWqVyojL7ZrKb8EH7HLz8I/Dgbtl4Fv9pTmjWpL5bIl5Lsd39icntjYWJk/Z4Y0a1hLalUpK73feE3O/vE7pzCF8LpwHpGRkTJl4jhp2bS+1KxcVrp0fFmCj/Ie9cjdzfbaXBSVRQqqVb6IzFu5S+p0nCzP9pwladJ4yKa5fSRD+nTGsX071JPYWPM53N3dZO2MnpIubRqp13mKdBv+sbzSqooM79kiZf4RFhIeHiZdOr0sadKkkRlzPpTV676UAYOHSmZvH0c3LdWLunVLnipaTIb4v/fA+5ctWSgrV3wibw8bKYs+XileXhnkzV7dJDo6OsXbajW8LpzLmJHvyo+BP8iosRPk0883SNVqNaT3G110IQ88LsYkpqDWfebY3O4+4hM5t2O8lCuZT/YcPB2/v3TRJ6Xfq/WlRoeJ8vs3ATaPaVithJQolFta9JgpIdduyM+//imj53wpY95sLWPmbZY7d++l2L8ntVuyaKHkyuUrI9//5xo8mTevQ9tkFdVr1tbbg6gU8bPly6RLtx5Sp14DvW/k++OlaYOasvPbb6RxU/5gsideF84jKipKvt2+TSZPmyXlK1TS+7r37CPf7/xW1qz+VHr26e/oJroWxiQ6V5J45coVmThxorRt21aqVaumN/X9pEmTJDQ0VFI770zp9df/hd2M3+eVPq0sCegs/cevkstXbxiPqVK6oBw9dUEXiHG2/XBMfDJ7ScnCvinUcmvY9d0OKfn0M/LWoH7SsE51+b8X28raz1c5ulmWd+HP83L1yhWpXKVa/LnIlDmzPF2qtBw5fNjy58feeF04j3v37uktnaenzX5Pz/Ry6KeDDmsXUg+HFYn79++XokWLyowZM8THx0dq166tN/W92le8eHE5cODAfz6P6l4KDw+32WJjnD9Nc3Nzk0mDn5cffjotv5y+GL9/4qDnZO/hM7LpuwePKcmVzVtC7iseQ66F/3Vfdm87t9pa/jx/Tj5f9an4+eWXWfMWyvMvtpfJE8bKxg3rHN00S1MFopI1Wzab/VmzZperV1P/H5eOxuvCeWTMmFFKlSkrHy2YK6EhIbpg3LzpCzny8yG5YoGgJdkxJtF5upv79u0rL7zwgsybN08XTPd3J/Xo0UMfExgY+K/PExAQIKNGjbLZ55GrkqT1rSzObJr/i/J0EV9p8NrU+H0t6pSSupWLStX24x3aNvwlJiZWSj79tPTpN1DfLl6ipJw6dVLWrP5MWrZuy2mCJfG6cC6jx06Q0SOGSfNGdcTDw0OKFS+ph1wcPxbs6KYhFXBYknj48GEZMGCAUSAqap+679ChQ//5PP7+/hIWFmazpclVQZzZ1KEvSPNaz0iTbjPkz5Dr8fvrVioqhfJml0u7JsmN/dP1pnw6+XXZ+mE//f3lq+GSM1tmm+fLmfWvBPHylb8SRSSP7DlySMFCRWz2FSxYWC5d+if5RcrLlj27/nrt6lWb/deuXZFs2XJwSeyM14VzyZvPTxYs+lh2BQbJpq07ZOmKVXL37h3GTz/qmER7bS7KYUli7ty5Zd++fbpb+UHUfbly5frP5/H09NRbQm7uHuLMBWKr+mWkcbfp8scF219ykxd/LYvX/WCzL+jzYfLWlDXy5c6j+vaPP5+RoV2bSI4nMkno/yL0vgZVi0vYjVty7LdLKfgvSf3KlC0nf/x+xmafWmbF1zePw9oEkTxP5tWF4v59e6Vo8RL6lEREREjwkZ/luRfac4rsjNeFc/LKkEFvavb53sA90rf/YEc3yfW4cDGX6orEwYMHS/fu3SUoKEgaNGgQXxBevnxZtm/fLh9++KFMnjxZUhPVxfxSs4rywoAFEhEZJbn+TgTDIqIkKvqOnqjyoMkq5y7+L76g/CbwmC4GPxrTSYZNX6/HKI7o/azMX7VLbt+5m+L/ptSsw6ud5bWOL8uiD+dJoybN5OiRn/XElWEjRju6aanezZuRcv7sWZvJKr8ePybePj6S2zePtO/QUV+XfH75ddE4b/YMyZ4jp9Sp19Ch7bYCXhfOJXDPbomVWMmfv6CcP/eHTJ86WQoUKCitGBKDZOAWqwYAOsjKlStl6tSpulBUA24VNaaiQoUKMnDgQHnxxRcf6Xm9yvURZ6QWxn4QtdbhJxt/fOhjXhywQDZ+93P8Pj/fJ2T6O+2ldoWnJDIqWpZv3Cfvztgg9+7FiLMJ3TtTXNmund/KrOkfyLmzf+hiRP2CbPf8o/1/6QzuxTjs5Z4kQfv3Sc9unYz9LVq2kRHvB+hxywvmzpR1a1ZLxI1wKVOuvLz1znD9i9JVeLi77gK7qe11EeO4X4OPbdvWr2T2jKkScvmS/iOqfoPG0qtvfz3j3xV5p3dcmufVaq7dnvvWFz3FFTm0SIxz584dvRyOkj17dkmbNu1jPZ+zFolW5OpFYmrjKkWiFbhykZjauHKRmNpQJDoXp1hMWxWFvr6s8QcAAByEMYkGRmkCAADAOZNEAAAAh3rAknxWR5IIAAAAA0kiAAAAYxINFIkAAAB0NxvobgYAAICBJBEAAFieG0migSQRAAAABpJEAABgeSSJJpJEAAAAGEgSAQAAWEvbQJIIAAAAA0kiAACwPMYkmigSAQCA5VEkmuhuBgAAgIEkEQAAWB5JookkEQAAAAaSRAAAYHkkiSaSRAAAABhIEgEAAFhM20CSCAAA4CQCAgKkUqVKkjlzZsmZM6e0adNGTpw4YXNMVFSU9O7dW7JlyyaZMmWS5557Ti5fvmxzzNmzZ6VFixaSIUMG/TxDhgyRu3fvJqktFIkAAMDy1JhEe21JsXPnTl0A7t27V7Zt2yZ37tyRxo0bS2RkZPwxAwYMkI0bN8rq1av18RcuXJB27drF33/v3j1dIN6+fVt++OEHWbp0qSxZskSGDx+epLa4xcbGxkoq41Wuj6ObgL+F7p3JuXAi92JS3cvdZXm407flLGJS369Bl+Wd3nHZVZYOn9jtua8vf+WRHxsaGqqTQFUM1q5dW8LCwiRHjhyyYsUKef755/Uxx48flxIlSkhgYKBUrVpVvvrqK3n22Wd18ZgrVy59zLx582To0KH6+dKlS5eon02SCAAALM+eSWJ0dLSEh4fbbGpfYqiiUMmaNav+GhQUpNPFhg0bxh9TvHhx8fPz00Wior6WKlUqvkBUmjRpon9ucHBwoq81RSIAALA8exaJAQEB4uPjY7Opff8lJiZG+vfvLzVq1JBnnnlG77t06ZJOArNkyWJzrCoI1X1xxyQsEOPuj7svsZjdDAAAYEf+/v4ycOBAm32enp7/+Tg1NvHo0aOye/ducQSKRAAAYHn2XEzb09MzUUVhQn369JFNmzbJrl27JG/evPH7c+fOrSekXL9+3SZNVLOb1X1xx+zbt8/m+eJmP8cdkxh0NwMAADiJ2NhYXSCuW7dOduzYIQULFrS5v0KFCpI2bVrZvn17/D61RI5a8qZatWr6tvp65MgRCQkJiT9GzZT29vaWkiVLJrotJIkAAABOsuBA79699czlDRs26LUS48YQqnGMXl5e+mvXrl1197WazKIKv759++rCUM1sVtSSOaoYfPXVV2XixIn6Od5991393ElJNCkSAQAAnMTcuXP117p169rsX7x4sXTu3Fl/P3XqVHF3d9eLaKtZ0mrm8pw5c+KP9fDw0F3VPXv21MVjxowZpVOnTjJ69OgktYV1EmFXrJPoXFgn0XmwTqLzYJ1E5+HIdRKzd/7Mbs99ZUl7cUWMSQQAAICB7mYAAGB59pzd7KooEgEAgOVRJJrobgYAAICBJBEAAIDeZgNJIgAAAAwkiQAAwPIYk2giSQQAAIA1ksRLP8xwdBPwt+i79zgXTsSdJR6cBn+hOw8W04ZCkmjifQoAAADWSBIBAACSgiTRRJEIAAAsjyLRRHczAAAADCSJAAAALKZtIEkEAACAgSQRAABYHmMSTSSJAAAAMJAkAgAAyyNJNJEkAgAAwECSCAAALI8k0USRCAAAwBI4BrqbAQAAYCBJBAAAlkd3s4kkEQAAAAaSRAAAYHkkiSaSRAAAABhIEgEAgOWRJJpIEgEAAGAgSQQAAJZHkmiiSAQAAGAxbQPdzQAAADCQJAIAAMuju9lEkggAAAADSSIAALA8kkQTSSIAAAAMJIkAAMDy3JjdbCBJBAAAgIEkEQAAWB5jEk0UiQAAwPLobjbR3QwAAAADSSIAALA8uptNJIkAAAAwkCQCAADLY0yiiSQRAAAABpJEAABgee7urKZ9P5JEAAAAGEgSAQCA5TEm0USRCAAALI8lcEx0NwMAAMBAkuhgB4P2yydLF8nxY8FyJTRUJn4wU+rWbxh/f2xsrCyYO1PWr10tETduSOmy5WToOyPEL38Bh7Y7Nfop6ICsWLZIThz7Ra5cCZWAKTOkTr0G8fePGfGObN64weYxVarVkKmzFzigtan/WnyirsUvwfpaTPhAXYt/Xhcfzpsl32z9Si5fuiRp06aVYiVKSo8+/eSZUmUc2m4raNm0gVy8cMHY/8JLL8vQYcMd0iZLvS5sfl/MkDoJfl98u32brF29Ut8fHhYmH3+2RooWL+HQNrsSuptNJIkOFnXrljxVtJgM8X/vgfcvW7JQVq74RN4eNlIWfbxSvLwyyJu9ukl0dHSKtzW1i4q6JUWKFpNBb7/70GOqVq8pG7/+Ln4bFTApRdtoFbdu3dSvi8EPeV2oP5IGDR0my1evl/mLPxbfPE9Kv17d5H/XrqV4W61m2YrVsmXHrvht9oKP9P4GjZs6ummWeV087PfFrVu3pEy58tKn36AUbxtSJ5JEB6tes7beHkSliJ8tXyZduvWIT7RGvj9emjaoKTu//UYaN22Rwq1N3arVqKW3f5M2XTrJlj1HirXJqv7tdaE0afasze3+g4bKxvVr5NTJE1KpSrUUaKF1PZE1q83tpR99KHnz+UmFipUc1iar+K/XRfNnW+mvF/78MwVblXowJtFEkujELvx5Xq5euSKVE/zSy5Q5szxdqrQcOXzYoW2zqp8O7JfmDWpJ+7YtZNK40RJ2/bqjm2R5d+7clvVrV0mmTJnlqaLFLX8+Uvrcb/5yo7Rq045fsEAq5NRJ4rlz52TEiBGyaNGihx6jul3v73qNjkkrnp6e4upUgahkzZbNZn/WrNnl6tVQB7XKuqpUr6nH/+TJk1fOnz8n82dNk4F935AFS1aIh4eHo5tnObt3fSfvvT1IoqKiJHv2HDJj3kLJ8sQTjm6WpXy3Y7seK92ydVtHNwV4bCSJLpYkXrt2TZYuXfqvxwQEBIiPj4/N9sGk8SnWRlhHoybNpVad+lL4qaK6+3/S9DlyLPioTheR8ipUqizLPlsrHy5ZoceKDntroFy7dpVLkYI2rFsj1WvUkhw5c3LegVTIoUniF1988a/3//bbb//5HP7+/jJw4ECbfVExaSU1yJY9u/567epVyZ7jnzfha9euSNGizFhztCfz5pMsWZ6Q8+fOSsUqVR3dHMtRk7jy+eXX2zOly8jzrZrKxnVrpFPX7o5umiVcvPCn7NsbKBOnznB0U4BkwexmJysS27Rpo+NdNUHjUeNf1a18f9dy7K0YSQ3yPJlXF4r79+2NX8YgIiJCgo/8LM+90N7RzbO8kMuXJCzsumTL8VcxD8dS7yO379zmMqSQL9av05NYataqwzlHqkB3s5MVib6+vjJnzhxp3br1A+8/dOiQVKhQQVKzmzcj5fzZszaTVX49fky8fXwkt28ead+hoyz6cJ5OS1TROG/2DJ0qJlwzDsl4Lc79cy0uqmtx4ph4e/vo67Fo/lyp26CRLtz/PHdOZk+fomd1VqlWk0tg52uhZmvGXQufLFlkycL5uutfXQs1eejzVSskNOSyNGjUhGuRAmJiYmTjhrXybKs2kiaNUw9tT+W/L/60+X2h/mi9fPGihIaG6Pv/+ON3/VW9TliVAY/Coa9uVQAGBQU9tEj8r5QxNTgWHCw9u3WKvz1tygT9tUXLNjLi/QDp2Pl1vZbiuPdHSMSNcL0G1vQ5C1LFxBxnc/yXYOnT/bX42zM+mKi/Nm/ZWob4D9fLq2zetEFfB1WoV65aXbr36ivp0qVzYKtTp2O/BEvvbp3jb0//+3XRvGUbGTpshPz++xnZvLGfXL/+P/HxySIlnn5G5i36WAoVfsqBrbYO1c186eJFPasZKfv7oleC10XC3xfD3x8n33/3rbw/Ylj8/e8O/Wu9xNff6CXdevbhUv0HuptNbrEOrMK+//57iYyMlKZNH7wIq7rvwIEDUqdO0rozwlJJd3NqcDeGa+FM3HkXdBppPP59KA1Szr2Y1B1GuJIsXo5bKaL86B12e+6Dw+uLK3Joklir1r8vXJwxY8YkF4gAAABJxZhEF1sCBwAAAI7BiGMAAGB5jMYxkSQCAADAQJIIAAAsjzGJJpJEAAAAGEgSAQCA5TEm0USRCAAALI/uZhPdzQAAADCQJAIAAMuju9lEkggAAAADSSIAALA8xiSaSBIBAABgIEkEAACWx5hEE0kiAAAADCSJAADA8hiTaKJIBAAAlkd3s4nuZgAAABhIEgEAgOXR3WwiSQQAAICBJBEAAFgeSaKJJBEAAAAGkkQAAGB5zG42kSQCAADAQJIIAAAsjzGJJopEAABgeXQ3m+huBgAAcCK7du2Sli1bSp48eXTCuX79epv7O3furPcn3Jo2bWpzzLVr16RDhw7i7e0tWbJkka5du0pERESS2kGRCAAALO/+osstGbekioyMlDJlysjs2bMfeowqCi9evBi/ffrppzb3qwIxODhYtm3bJps2bdKFZ/fu3ZPUDrqbAQAAnEizZs309m88PT0ld+7cD7zv2LFjsmXLFtm/f79UrFhR75s5c6Y0b95cJk+erBPKxCBJBAAAlqcCP3tt0dHREh4ebrOpfY/ju+++k5w5c0qxYsWkZ8+ecvXq1fj7AgMDdRdzXIGoNGzYUNzd3eXHH39M9M+gSAQAALCjgIAA8fHxsdnUvkelupqXLVsm27dvlwkTJsjOnTt18njv3j19/6VLl3QBmVCaNGkka9as+r7EorsZAABYnrsdpzf7+/vLwIEDje7iR9W+ffv470uVKiWlS5eWwoUL63SxQYMGklxIEgEAAOzI09NTzzJOuD1OkXi/QoUKSfbs2eXUqVP6thqrGBISYnPM3bt39Yznh41jfBCKRAAAYHn2HJNob+fPn9djEn19ffXtatWqyfXr1yUoKCj+mB07dkhMTIxUqVIl0c9LdzMAALA8Z/rElYiIiPhUUDlz5owcOnRIjylU26hRo+S5557TqeDp06flrbfekiJFikiTJk308SVKlNDjFrt16ybz5s2TO3fuSJ8+fXQ3dWJnNiskiQAAAE7kwIEDUq5cOb0pajyj+n748OHi4eEhP//8s7Rq1UqKFi2qF8muUKGCfP/99zZd2MuXL5fixYvrMYpq6ZuaNWvKggULktQOt9jY2FhJZcJuxTi6Cfjb3RiuhVUGZiNp0nhwLZzFvZhU92vQZWXx8nDYz242N/FLwyTVVz0T38XrTEgSAQAAYGBMIgAAsDxnGpPoLEgSAQAAYCBJBAAAlkeQaJEikQvtPCKj//qIIDiH06ERjm4C/vZ0Hm/OhZOIusMEO2fhyIkrsEiRCAAAkBRuwpjE+1EkAgAAy3OnRjQwcQUAAAAGkkQAAGB5LIFjIkkEAACAgSQRAABYHiujmEgSAQAAYCBJBAAAludOlGggSQQAAICBJBEAAFgeQaKJIhEAAFgeS+CY6G4GAACAgSQRAABYHt3NJpJEAAAAGEgSAQCA5bEEjokkEQAAAAaSRAAAYHlulj8DJpJEAAAAGEgSAQCA5bFOookiEQAAWJ47/c0GupsBAABgIEkEAACWR3eziSQRAAAABpJEAABgeXwsn4kkEQAAAAaSRAAAYHmMSTSRJAIAAMBAkggAACyPdRJNFIkAAMDy6G420d0MAAAAA0kiAACwPD6Vz0SSCAAAAANJIgAAsDx3VtM2kCQCAADg0ZPEdu3aJfZQWbt2baKPBQAAcDSCxMcoEn18fBJ7KAAAAKxSJC5evNi+LQEAAHAQ1kk0MSYRAAAAyTe7+fPPP5dVq1bJ2bNn5fbt2zb3HTx48FGfFgAAIMUxJjGZisQZM2bIsGHDpHPnzrJhwwZ57bXX5PTp07J//37p3bv3ozwl/hYZGSnzZk+X73Z8I/+7dk2KFi8hg956R55+phTnyI4+XbpQdu/cLuf+OCOenp5SslRZeb1Xf8mXv2D8MdeuXpEFsz6Qg/sC5dbNSMnrV0D+r3M3qVWvEdcmmf3vaqisWzJbgg/uldvRUZLDN690enOY5H+qhL6/R6vqD3xcu869pXG7DlyPZHTo4AH59OPFcuLYL3L1SqiMnTxdatdtEH//zZs3Zf7MqfL9zh0SFnZdfPM8Kc+/1EHaPP8S1yGZ8T5lXyyBk0xF4pw5c2TBggXy8ssvy5IlS+Stt96SQoUKyfDhw+XatWuP8pT425iR78rpUydl1NgJkiNHTvnqy43S+40usmrtJsmZKxfnyU5+/umAtHquvRQr8bTcu3dPFs2bIW/37yELV6wTL68M+pgJo4dJ5I0bMnriDPHJ8oTs+HqzjHl3iMxe9KkUKfZX8YLHFxkRLpOGviHFSpWXPiM+kMzeWSTk4jnJkClz/DETlm60eUxwUKB8PDNAylWvyyVIZlG3bkmRp4pJi1ZtZdiQ/sb9s6ZOlIP7f5T3RgdI7jxPyv69P8gHE8ZI9hw5pWadelyPZMT7FFxiTKLqYq5e/a+/5L28vOTGjRv6+1dffVU+/fTT5G2hhURFRcm327fJmwMGS/kKlSSfX37p3rOP5MvnJ2tWc17tKWDaPGnSorUUKFRECj9VTIa8+76EXLooJ4//En/ML0cOSesXXpbiT5cS3yfzSofXukvGTJnl1xP/HIPH9/WaTyRr9lzSqd+7UrBoScmeO4+ULFdFp4lxfJ7IZrMd/vF7KVqqvOTI/SSXIJlVrVFLuvV6U2rXa/jA+48ePiRNn20t5SpW1iliq3Yv6NfQseAjXItkxvuU/bub7bVZqkjMnTt3fGLo5+cne/fu1d+fOXNGYmNjk7eFFqISLLWl8/S02e/pmV4O/cQ4z5QUGRGhv2b2/mfpJ9UFvfObrRIeFiYxMTHy7bav5M7taClTrlKKti21O7xvt/gVKS4Lxg+TIa82l7H9Osn3Wzc89Pjw/12TIwd+kBqNWqZoO/GXZ8qUlT27vpXQkMv6/f/ggX1y7uzvUqnqg4cEIPnwPgWn7G6uX7++fPHFF1KuXDk9HnHAgAF6IsuBAweStOg2bGXMmFFKlSkrHy2YKwULFpas2bLJ1q++lCM/H5K8+fw4XSlEFYBzp02Up0uXk4KFn4rf/96YSTLmvbfkuaa1xMMjjXimTy8jxk+TJ7k2yerKpQuy66t10rB1e2n6Qkf54+QxWfXhVEmTJq1Ua9DcOD5wx2ZJ75VBylWrk7wNQaL0H/KOTBo7Uto1b6BfF+7ubvLWsJFStnxFzqAd8T6V/FgCJ5mKRDUeUf0PqqiJKtmyZZMffvhBWrVqJW+88UaSnuvWrVsSFBQkWbNmlZIlSxrdr2oGdceOHR/6+OjoaL3Z7ItNqycfuKLRYyfI6BHDpHmjOuLh4SHFipeUxk1byPFjwY5ummXMnDxWfv/tlEydv8Rm/5IFsyXyRrhMmLFAj0n8YdcOPSZx6tzFUrBIUYe1N7WJjY2R/EWKS5uOPfRtv8LF5MLZ32TXlnUPLBJ/+GaTVK7TRNKmc83XvKtbs3K5BB/5WcZ/MEty+frK4YNB8sHEsXpMYsUq1RzdvFSL9yk4bXezu7u7pEnzT33Zvn17PeO5b9++ki5dukQ/z6+//iolSpSQ2rVrS6lSpaROnTpy8eLF+PvDwsJ0UvlvAgIC9KfBJNw+mDReXJVKDBcs+lh2BQbJpq07ZOmKVXL37h15Mu8/47FgPzMnj5Mf9+ySSbMXSo6cueP3Xzh/TjZ8/qkMGjZayleqqsdcvdq1pxQtXlI2rFnJJUlGaoyhb75/ZpUrufMWkGuhl41jTwYfkst/npWajelqdoToqChZMHu69Bk4RGrUrqsnuDz30v9J/UZN5dNPbP/IQvLhfcp+BZG9Nlf1yG3//vvv5ZVXXpFq1arJn3/+qfd9/PHHsnv37kQ/x9ChQ+WZZ56RkJAQOXHihGTOnFlq1KihJ8Yklr+/vy4mE24Dh7wtrs4rQwb9l3h4eJjsDdxjs+QEkp8aS6XeePfs3CETZy0U3zy2RXl01C391c3d9iXj7uGhky8kn8IlSuvCL6HLF85JtgRFe5w92zbp8Yt5C/4zLAAp5+7du3pzd7N9XXi4e0js371NSD68T8ElisQ1a9ZIkyZN9Mzmn376Kb67VxVo48aNS/TzqC5qlQRmz55dihQpIhs3btTPW6tWLfntt98S9RyqW9nb29tmc9WuZiVwz275Yc/38uf58/Jj4B7p8XpnKVCgoLRq3dbRTUv1XTfbt34p/qPGS4YMGfWaiGpTSYmSr0BByZPXT6ZPGC3Hg4/oZHH1iqV6zcTqtes7uvmpSoPWL8lvJ47KV6uWSsiF87Jv59eye+sGqdP8OZvj1FqVB/fskJpMWLErtQ7iyRPH9aZc/PNP/f3lSxclY6ZMeuzhnOlT5KcD++TCn+dl88b1smXzF1KrHn/YJjfep+w/JtFem6tyi32E6chqwoqarKLGCqr07/Dhw3qdRFUwNmvWTC5dupSo51EF3Y8//qi7nBPq06ePXqR7xYoVUrduXT3jNynCo1z3L9htW7+S2TOmSsjlS+Lt4yP1GzSWXn37S6bM/6wR50qu37wjrqBRtdIP3D/43ff10jjK+XN/yEdzpsnRwz9J1K2bumh8/v86SaNmrtPVeTr0r1nbzu7n/Xtk/bK5ukjMnstXGrRuL7Wa/HUd4ny/Zb2sWjhdJi7dKF4ZM4mreTqPt7gCVfy92aOLsV8tezNs5Fi5euWKzJ89Ta+PqHo+cufOIy3bPi8vdejoMr8co+64xu8MK7xP+WV1XMjTf8NffwjZw7TWxcUyRWKGDBnkl19+kQIFCtgUiSr9U5NP1ISTxKhcubIex6jWV7yfKhSXL18u4eHhlioSUxtXKRKtwlWKRCtwlSLRClylSLQCisRUsk7iqVOnjP1qPKIqFhOrbdu2D118e9asWfoTXVh3EQAA2Ju7m/02V/VIRWK3bt2kX79+uqtYdSdcuHBBp36DBg2Snj17JmnSyebNm//14//iltoBAACAk6+T+Pbbb+virUGDBnpQs1rCRk0WGTJkiLz++uvJ30oAAAA7cpUxtE6fJKoTOWzYMP3RfEePHtUfyxcaGqrXKCxY0HZ9MwAAAKTyIlEtdaO6iCtWrKjXM1RdxWqiSnBwsBQrVkymT5+uZz0DAAC4EsYkPmZ38/Dhw2X+/PnSsGFDvcbhCy+8oD8RRSWJU6ZM0bfVR8kBAADAtSWpSFy9erUsW7ZMf0az6mYuXbq0Xm1fLYFDXz4AAHBVDEl8zCLx/PnzUqFCBf29+jg9NVlFdS9TIAIAAFfmTpX4eGMS1aLW6dKli7+dJk0ayZTJ9T7pAAAAAMmYJKqFrTt37hz/2cjqk1V69OghGTNmtDlu7dq1SXlaAAAA11vuJZVLUpHYqVMnm9uvvPJKcrcHAAAArlYkLl682H4tAQAAcBCGJJpIVwEAAJA8H8sHAACQmjC72USSCAAAAANJIgAAsDzGJJooEgEAgOWpz26GLbqbAQAAYCBJBAAAlsfEFRNJIgAAAAwkiQAAwPKYuGIiSQQAAICBJBEAAFges5tNJIkAAAAwkCQCAADLcxMWSrwfRSIAALA8uptNdDcDAADAQJIIAAAsjyTRRJIIAAAAA0kiAACwPDdW0zaQJAIAADiRXbt2ScuWLSVPnjy6eF2/fr3N/bGxsTJ8+HDx9fUVLy8vadiwoZw8edLmmGvXrkmHDh3E29tbsmTJIl27dpWIiIgktYMiEQAAWJ4ak2ivLakiIyOlTJkyMnv27AfeP3HiRJkxY4bMmzdPfvzxR8mYMaM0adJEoqKi4o9RBWJwcLBs27ZNNm3apAvP7t27J6kdbrGqHE1lwqNiHN0E/O36zTucCydyOjRpf0XCfp7O483pdRJRd/id4Sz8sno67GdP2fmb3Z57UJ1Cj/xYlSSuW7dO2rRpo2+rsk0ljIMGDZLBgwfrfWFhYZIrVy5ZsmSJtG/fXo4dOyYlS5aU/fv3S8WKFfUxW7ZskebNm8v58+f14xODJBEAAFieGpJory06OlrCw8NtNrXvUZw5c0YuXbqku5jj+Pj4SJUqVSQwMFDfVl9VF3Ncgaio493d3XXymFgUiQAAwPLc3dzstgUEBOhCLuGm9j0KVSAqKjlMSN2Ou099zZkzp839adKkkaxZs8YfkxjMbgYAALAjf39/GThwoM0+T0/Hda0nFkUiAACwPHsupu3p6ZlsRWHu3Ln118uXL+vZzXHU7bJly8YfExISYvO4u3fv6hnPcY9PDLqbAQAAXETBggV1obd9+/b4fWqMoxprWK1aNX1bfb1+/boEBQXFH7Njxw6JiYnRYxcTiyQRAABYnjOtpR0RESGnTp2ymaxy6NAhPabQz89P+vfvL2PGjJGnnnpKF43vvfeenrEcNwO6RIkS0rRpU+nWrZteJufOnTvSp08fPfM5sTObFYpEAAAAJ3LgwAGpV69e/O248YydOnXSy9y89dZbei1Fte6hSgxr1qypl7hJnz59/GOWL1+uC8MGDRroWc3PPfecXlsxKVgnEXbFOonOhXUSnQfrJDoP1kl0Ho5cJ3H2nt/t9ty9axQQV5Qqk8S0Hgy1dBbZMqVzdBOQQJYMT3A+gPtk8kyVvwqBx8YrAwAAWJ4zjUl0FhSJAADA8uy5BI6rol8WAAAABpJEAABgeerj82CLJBEAAAAGkkQAAGB5BIkmkkQAAAAYSBIBAIDlMSbRRJIIAAAAA0kiAACwPMYkmigSAQCA5dG1auKcAAAAwECSCAAALM+N/mYDSSIAAAAMJIkAAMDy+FA+E0kiAAAADCSJAADA8lhM20SSCAAAAANJIgAAsDzGJJooEgEAgOWxAo6J7mYAAAAYSBIBAIDlsZi2iSQRAAAABpJEAABgeaRmJs4JAAAADCSJAADA8hiTaCJJBAAAgIEkEQAAWB6LaZtIEgEAAGAgSQQAAJbHmEQTRSIAALA8ulZNnBMAAAAYSBIBAIDl0d1sIkkEAACAgSQRAABYHkvgmEgSAQAAYCBJBAAAludGlGggSQQAAICBJBEAAFieO6MSDRSJAADA8uhuNtHd7GQ++nC+/N9Lz0n1yuWkXu1q0v/NXvL7md8c3SxLmjdnppQvVdxma9eymaObZVmRkZEyZeI4adm0vtSsXFa6dHxZgo8ecXSzLIlr4ZwWLVwg5UoVl0kTxjm6KUglSBKdTNCBffLSyx3k6WdKyb2792Tm9A+kZ/eusnbDl+KVIYOjm2c5hYs8JXM/XBR/28ODl4yjjBn5rpw+dVJGjZ0gOXLklK++3Ci93+giq9Zukpy5cjmsXVbEtXA+6g+mNZ+vlKeKFnN0U1yWG93NBpJEJzNn/kfSuk07KVLkKSlWvLiMHjteLl68IL/8EuzoplmSh4eHZM+eI3574oknHN0kS4qKipJvt2+TNwcMlvIVKkk+v/zSvWcfyZfPT9as/tTRzbMUroXzuXkzUt55e7C8N+J98fb2dnRzkIpQJDq5iIgb+quPj4+jm2JJZ8/+IY3r15KWTRvKsKGDdcGOlHfv3j29pfP0tNnv6ZleDv10kEvCtbC0gLGjpVatulK1WnVHN8XlxyTaa3NVDi8Sjx07JosXL5bjx4/r2+prz549pUuXLrJjx47/fHx0dLSEh4fbbGpfahATEyOTxo+TsuXKS5Gnijq6OZZTqlQZGfV+gMyau1D83xshf/55Xrp2ekUiIyMc3TTLyZgxo5QqU1Y+WjBXQkNCdMG4edMXcuTnQ3IlNNTRzbMUroVz2fLVl3L8l1+kb/+Bjm4KUiGHFolbtmyRsmXLyuDBg6VcuXL6du3ateXUqVPyxx9/SOPGjf+zUAwICNApW8Jt0oQASQ0CxoySU6dOyoRJUx3dFEuqUau2NGrSVIoWKybVa9SSmXMWSMSNcNm2dYujm2ZJo8dOkNjYWGneqI7UqFRGVq74RBo3bSHu7g7/W9dyuBbO4dKlizpIGDt+snjel7Lj0ZbAsdfmqtxi1buug1SvXl3q168vY8aMkc8++0x69eqlU8SxY8fq+/39/SUoKEi+/vrrhz6HSg3vTw5j3D1d/gWjug++27FdFi39RJ7Mm09cVYzj/veyi1faPy9VqlaTvv0HiSu6F+P61+PWzZs6zc2eI6f4Dxkgt27dlGmz5ju6WZaUWq5FGnfX/CX+7fZvZGD/PnrsdByVsru5uek/nn4M+tnmPleQIZ3jrsWWYPv1SjR9Ooe4IodO1QwODpZly5bp71988UV59dVX5fnnn4+/v0OHDror+t+oYvD+gvDWHXFZqmYfP+592bF9myxc/LFLF4ipcXD4+XPnpEXLVo5uiqWpWf5qCw8Pk72Be6Rv/8GObpJlcS0cq3LVqrJ67Rc2+0a8944ULFhIOnd53eUKREdz5bGD9uLw9TzUXzyK+qsnffr0NhM0MmfOLGFhYWIl48aMkq82b5JpM+bosT9Xrvz1l02mTJn1+UHKmTp5gtSuU0988+SR0NAQmTd7lrh7uEvTZs9yGRwgcM9uiZVYyZ+/oJw/94dMnzpZChQoKK1at+V6cC0sKWPGTMZ4dS8vL/HJkoVx7I+AItHJisQCBQrIyZMnpXDhwvp2YGCg+Pn5xd9/9uxZ8fX1FStZvfKv5Txef+1Vm/2jxgTopXGQci5fviz+QwdJ2PXr8sQTWaVs+QqydPlKeSJrVi6Dg2b6z54xVUIuXxJvHx+p36Cx9OrbX9KkTcv14FoASG1jEufNmyf58uWTFi1aPPD+d955R0JCQmThwoVJel5X7m5ObVLbmERXlxrGJALJzVXHJKZGjhyTuO3YFbs9d6MS2cUVObRItBeKROdBkehcKBIBE0Wi86BIdC4OH5MIAADgaATKJhYYAwAAgIEkEQAAWJ6bCy96bS8kiQAAADCQJAIAAMtjnUQTRSIAALA8uptNdDcDAADAQJIIAAAsjyVwTCSJAAAAMJAkAgAAy2NMookkEQAAAAaSRAAAYHksgWMiSQQAAICBJBEAAFgeH8pnokgEAACW505/s4HuZgAAABhIEgEAgOXR3WwiSQQAAICBJBEAAIAo0UCSCAAAAANJIgAAsDw+ls9EkggAAAADSSIAALA8lkk0USQCAADLY96Kie5mAAAAGEgSAQAAiBINJIkAAAAwkCQCAADLYwkcE0kiAAAADCSJAADA8lgCx0SSCAAAAANJIgAAsDwmN5soEgEAAKgSDXQ3AwAAOImRI0eKm5ubzVa8ePH4+6OioqR3796SLVs2yZQpkzz33HNy+fJlu7SFIhEAAFiemx3/S6qnn35aLl68GL/t3r07/r4BAwbIxo0bZfXq1bJz5065cOGCtGvXTuyB7mYAAAAnkiZNGsmdO7exPywsTD766CNZsWKF1K9fX+9bvHixlChRQvbu3StVq1ZN1naQJAIAAMtTS+DYa4uOjpbw8HCbTe17mJMnT0qePHmkUKFC0qFDBzl79qzeHxQUJHfu3JGGDRvGH6u6ov38/CQwMDDZryFFIgAAgB0FBASIj4+Pzab2PUiVKlVkyZIlsmXLFpk7d66cOXNGatWqJTdu3JBLly5JunTpJEuWLDaPyZUrl74vudHdDAAALM+ek5v9/f1l4MCBNvs8PT0feGyzZs3ivy9durQuGvPnzy+rVq0SLy+vFL1OqbJIZNV053HvXqyjm4AEbkbf43w4CW+vVPn265LuxvA+5TxS5zo0np6eDy0K/4tKDYsWLSqnTp2SRo0aye3bt+X69es2aaKa3fygMYyPi+5mAAAANztujyEiIkJOnz4tvr6+UqFCBUmbNq1s3749/v4TJ07oMYvVqlWT5MafsgAAwPIeZakaexg8eLC0bNlSdzGr5W1GjBghHh4e8vLLL+uxjF27dtVd11mzZhVvb2/p27evLhCTe2azQpEIAADgJM6fP68LwqtXr0qOHDmkZs2aenkb9b0ydepUcXd314toqxnSTZo0kTlz5tilLW6xsbGpbjBG1F1HtwBxbt+N4WQ4EcYkOg/GJDoPxiQ6D+/0jhsFd+R8hN2eu1TeTOKKGJMIAAAAA93NAADA8pxjRKJzIUkEAACAgSQRAACAKNFAkggAAAADSSIAALA8Z1kn0ZmQJAIAAMBAkggAACzPjSDRQJEIAAAsjxrRRHczAAAADCSJAAAARIkGkkQAAAAYSBIBAIDlsQSOiSQRAAAABpJEAABgeSyBYyJJBAAAgIEkEQAAWB6Tm00UiQAAAFSJBrqbAQAAYCBJBAAAlscSOCaSRAAAABhIEgEAgOWxBI6JJBEAAAAGkkQAAGB5TG42kSQCAADAQJIIAABAlGigSAQAAJbHEjgmupsBAABgIEkEAACWxxI4JpJEAAAAGEgSAQCA5TFvxUSSCAAAAANJIgAAAFGigSQRAAAABpJEAABgeayTaKJIBAAAlscSOCa6m51M0IH90rdXD2lYt6aUebqY7Nj+jaObZGmRkZEyZeI4adm0vtSsXFa6dHxZgo8ecXSzUrVPlnwo3Tu9JE3rVpbWTWrLsMFvytk/ztgc88W61dKvR2dpVq+K1Kn8jNy4Ee6w9lrdooULpFyp4jJpwjhHN8WSeI+CPVEkOplbt25KsWLFxP/dEY5uCkRkzMh35cfAH2TU2Any6ecbpGq1GtL7jS4Scvky58dODh88IG1feFnmfrRCpsxcIHfv3ZHBfbvr10ac6KgoqVytprzSuRvXwYHUH0xrPl8pTxUtxnVwEN6jknfeir02V0WR6GRq1qojffoNkAYNGzm6KZYXFRUl327fJm8OGCzlK1SSfH75pXvPPpIvn5+sWf2p5c+PvUyaMV+aPdtGChYuIkWKFhf/4WPl8qWL8uuxX+KPeeHlV6VDp9el5DOluQ4OcvNmpLzz9mB5b8T74u3tzXVwAN6jYLkiMTY21tFNALR79+7pLZ2np80Z8fRML4d+OshZSiERERH6a2YfH865EwkYO1pq1aorVatVd3RTLIv3qOQfk2ivzVU5XZHo6ekpx44dc3QzAMmYMaOUKlNWPlowV0JDQvQb8uZNX8iRnw/JldBQzlAKiImJkVkfjJdSZcpJocJPcc6dxJavvpTjv/wiffsPdHRTLI33KKTa2c0DBz74zUX9Ih4/frxky5ZN3/7ggw/+9Xmio6P1llCsh6cuNoHHNXrsBBk9Ypg0b1RHPDw8pFjxktK4aQs5fiyYk5sCpk4cI2d+OyUzFyzjfDuJS5cuyqTx42TugkW8zzoB3qOSkwtHfqmtSJw2bZqUKVNGsmTJYnQ3qyRR/YXkloiMNiAgQEaNGmWzb9h7I+Td4SOTvc2wnrz5/GTBoo/l1s2bEhkZIdlz5BT/IQPkybx5Hd20VG/apLESuHunzJy/VHLmyu3o5uBvx4KD5dq1q/J/L7Wz+eP+YNABWfnpcvkx6Gf9BxVSBu9RSJVF4rhx42TBggUyZcoUqV+/fvz+tGnTypIlS6RkyZKJeh5/f38jlVRJIpCcvDJk0Ft4eJjsDdwjffsP5gTbifpDcfrkcfL9d9tl+tzF4vskBbkzqVy1qqxe+4XNvhHvvSMFCxaSzl1ep0B0EN6jHp8rjx1MdUXi22+/LQ0aNJBXXnlFWrZsqRNBVSAmlepWvr9rOequuKybkZFy9uzZ+Nt/nj8vx48dEx8fH/HNk8ehbbOiwD27JVZiJX/+gnL+3B8yfepkKVCgoLRq3dbRTUvVXczbt26WsZNniFeGjHL1yhW9P1OmTOKZPr3+Xu27du2K/Hnur9fKb6dOSoaMGSVXLl/xZoKLXWXMmEmKPFXUZp+Xl5f4ZMli7If98R6VfKgRnewTVypVqiRBQUHSu3dvqVixoixfvjxRXcypWXDwUXn9tY7xtydPDNBfVVHy/rjxDmyZNUVE3JDZM6ZKyOVLuvio36Cx9OrbX9I8wh80SJwNa1bqr/16vGaz/+3hY/TSOMoXa1fKkoVz4+97841OxjGAFfAeBXtyi3WSNWc+++wz6d+/v4SGhsqRI0cS3d38IK6cJKY2t+/GOLoJSOBm9D3Oh5Pw9uJTUZ3F3Rin+DUI9bpI77hFVy6G3bbbc/v6pBNX5DRFonL+/HmdLDZs2FBPXHlUFInOgyLRuVAkOg+KROdBkeg8KBKdi1P9KZs3b169AQAApCQ3RiU6/2LaAAAAcDynShIBAAAcwtrzZh+IJBEAAAAGkkQAAGB5BIkmikQAAGB5Fl+m+YHobgYAAICBJBEAAFgeS+CYSBIBAABgIEkEAABgTKKBJBEAAAAGkkQAAGB5BIkmkkQAAAAYSBIBAIDlsU6iiSIRAABYHkvgmOhuBgAAgIEkEQAAWB7dzSaSRAAAABgoEgEAAGCgSAQAAICBMYkAAMDyGJNoIkkEAACAgSQRAABYHuskmigSAQCA5dHdbKK7GQAAAAaSRAAAYHlulj8DJpJEAAAAGEgSAQAAiBINJIkAAAAwkCQCAADLYwkcE0kiAAAADCSJAADA8lgn0USSCAAAAANJIgAAsDwmN5soEgEAAKgSDXQ3AwAAwECSCAAALI8lcEwkiQAAADCQJAIAAMtjCRwTSSIAAAAMbrGxsbHmbjhadHS0BAQEiL+/v3h6ejq6OZbGtXAeXAvnwbVwLlwP2ANFopMKDw8XHx8fCQsLE29vb0c3x9K4Fs6Da+E8uBbOhesBe6C7GQAAAAaKRAAAABgoEgEAAGCgSHRSarLKiBEjmLTiBLgWzoNr4Ty4Fs6F6wF7YOIKAAAADCSJAAAAMFAkAgAAwECRCAAAAANFIgAAAAwUiU5o9uzZUqBAAUmfPr1UqVJF9u3b5+gmWdKuXbukZcuWkidPHnFzc5P169c7ukmWpT6islKlSpI5c2bJmTOntGnTRk6cOOHoZlnS3LlzpXTp0vqToNRWrVo1+eqrrxzdLIjI+PHj9XtV//79OR9IFhSJTmblypUycOBAvfzNwYMHpUyZMtKkSRMJCQlxdNMsJzIyUp9/VbTDsXbu3Cm9e/eWvXv3yrZt2+TOnTvSuHFjfY2QsvLmzauLkaCgIDlw4IDUr19fWrduLcHBwVwKB9q/f7/Mnz9fF/BAcmEJHCejkkOVmMyaNUvfjomJkXz58knfvn3l7bffdnTzLEv9db5u3TqdYMHxQkNDdaKoisfatWs7ujmWlzVrVpk0aZJ07drV8ufCESIiIqR8+fIyZ84cGTNmjJQtW1amTZvGtcBjI0l0Irdv39Z/nTds2DB+n7u7u74dGBjo0LYBziQsLCy+OIHj3Lt3Tz777DOd6KpuZziGStlbtGhh87sDSA5pkuVZkCyuXLmi33Rz5cpls1/dPn78OGcZ+DtdV2OuatSoIc888wznxAGOHDmii8KoqCjJlCmTTtlLlizJtXAAVaSroUmquxlIbhSJAFwuNTl69Kjs3r3b0U2xrGLFismhQ4d0ovv5559Lp06ddNc/hWLKOnfunPTr10+P01UTHYHkRpHoRLJnzy4eHh5y+fJlm/3qdu7cuR3WLsBZ9OnTRzZt2qRnnqsJFHCMdOnSSZEiRfT3FSpU0CnW9OnT9cQJpBw1PElNalTjEeOo3ij1+lDj2qOjo/XvFOBRMSbRyd541Rvu9u3bbbrW1G3G+8DKYmNjdYGoujV37NghBQsWdHSTkIB6n1IFCVJWgwYNdNe/SnXjtooVK0qHDh309xSIeFwkiU5GLX+jum7UC71y5cp6hpoaFP7aa685ummWnDF46tSp+NtnzpzRb7xqsoSfn59D22bFLuYVK1bIhg0b9FqJly5d0vt9fHzEy8vL0c2zFH9/f2nWrJl+Ddy4cUNfl++++062bt3q6KZZjnot3D8uN2PGjJItWzbG6yJZUCQ6mZdeekkv7zF8+HD9i1AtZbBlyxZjMgvsT60BV69ePZsCXlFF/JIlS7gEKbyAs1K3bl2b/YsXL5bOnTtzLVKQ6t7s2LGjXLx4URfpal0+VSA2atSI6wCkMqyTCAAAAANjEgEAAGCgSAQAAICBIhEAAAAGikQAAAAYKBIBAABgoEgEAACAgSIRAAAABopEAAAAGCgSATgt9Wkqbdq0ib+tPnGlf//+Kd4O9bFzbm5ucv369RT/2QDgKBSJAB6peFNFk9rSpUsnRYoUkdGjR8vdu3ftejbXrl0r77//fqKOpbADgMfDZzcDeCRNmzbVn50cHR0tmzdvlt69e0vatGnF39/f5rjbt2/rQjI5ZM2alasFACmEJBHAI/H09JTcuXNL/vz5pWfPntKwYUP54osv4ruIx44dK3ny5JFixYrp48+dOycvvviiZMmSRRd7rVu3lt9//z3++e7duycDBw7U92fLlk3eeustiY2NtfmZ93c3qwJ16NChki9fPt0elWh+9NFH+nnr1aunj3niiSd04qnapcTExEhAQIAULFhQvLy8pEyZMvL555/b/BxV9BYtWlTfr54nYTsBwCooEgEkC1VQqdRQ2b59u5w4cUK2bdsmmzZtkjt37kiTJk0kc+bM8v3338uePXskU6ZMOo2Me8yUKVNkyZIlsmjRItm9e7dcu3ZN1q1b968/s2PHjvLpp5/KjBkz5NixYzJ//nz9vKpoXLNmjT5GtePixYsyffp0fVsViMuWLZN58+ZJcHCwDBgwQF555RXZuXNnfDHbrl07admypRw6dEhef/11efvtt/m/BIDl0N0M4LGotE8VhVu3bpW+fftKaGioZMyYURYuXBjfzfzJJ5/oBE/tU6meorqqVWqoxg42btxYpk2bpruqVYGmqCJOPefD/Prrr7Jq1SpdiKoUUylUqJDRNZ0zZ079c+KSx3Hjxsk333wj1apVi3+MKkpVgVmnTh2ZO3euFC5cWBetikpCjxw5IhMmTOD/FACWQpEI4JGohFCldiolVAXg//3f/8nIkSP12MRSpUrZjEM8fPiwnDp1SieJCUVFRcnp06clLCxMp31VqlT5580pTRqpWLGi0eUcR6V8Hh4eurBLLNWGmzdvSqNGjWz2qzSzXLly+nuVSCZshxJXUAKAlVAkAngkaqyeSt1UMajGHqqiLo5KEhOKiIiQChUqyPLly43nyZEjxyN3byeVaofy5ZdfypNPPmlznxrTCAD4B0UigEeiCkE1USQxypcvLytXrtRdv97e3g88xtfXV3788UepXbu2vq2W0wkKCtKPfRCVVqoEU40ljOtuTiguyVQTYuKULFlSF4Nnz559aAJZokQJPQEnob179ybq3wkAqQkTVwDYXYcOHSR79ux6RrOauHLmzBk9FvHNN9+U8+fP62P69esn48ePl/Xr18vx48elV69e/7p4dYECBaRTp07SpUsX/Zi451TjFBU161qNf1Td4mqcpEoRVXf34MGD9WSVpUuX6q7ugwcPysyZM/VtpUePHnLy5EkZMmSInvSyYsUKPaEGAKyGIhGA3WXIkEF27dolfn5+emKKSuu6du2qxyTGJYuDBg2SV199VRd+agygKujatm37r8+ruruff/55XVAWL15cunXrJpGRkfo+1Z08atQoPTM5V65c0qdPH71fLcb93nvv6VnOqh1qhrXqflZL4iiqjWpmtCo81fI4agKNmuwCAFbjFvuwUeEAAACwLJJEAAAAGCgSAQAAYKBIBAAAgIEiEQAAAAaKRAAAABgoEgEAAGCgSAQAAICBIhEAAAAGikQAAAAYKBIBAABgoEgEAACA3O//AbPuuwyZk/nHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    val_predictions = model(x_val_tensor)\n",
    "    val_pred_classes = torch.argmax(val_predictions, dim=1)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_val_tensor.numpy(), val_pred_classes.numpy())\n",
    "print(f\"Val accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "print(classification_report(y_val, val_pred_classes.numpy()))\n",
    "\n",
    "confMatrix = confusion_matrix(y_val, val_pred_classes.numpy())\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confMatrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c831d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, epochs=200, lr=1e-3, print_epoch_every=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        predictions = model(x_train)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(predictions, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if epoch % print_epoch_every == 0  or epoch == epochs - 1:\n",
    "            print(f\"Epoch: {epoch}; train loss={loss.item():.4f};\")\n",
    "\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "488a83cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train images...\n",
      "Loaded 3662 size of train data\n",
      "Epoch: 0; train loss=0.7409;\n",
      "Epoch: 10; train loss=0.7378;\n",
      "Epoch: 20; train loss=0.7352;\n",
      "Epoch: 30; train loss=0.7325;\n",
      "Epoch: 40; train loss=0.7298;\n",
      "Epoch: 50; train loss=0.7270;\n",
      "Epoch: 60; train loss=0.7241;\n",
      "Epoch: 70; train loss=0.7212;\n",
      "Epoch: 80; train loss=0.7181;\n",
      "Epoch: 90; train loss=0.7149;\n",
      "Epoch: 100; train loss=0.7116;\n",
      "Epoch: 110; train loss=0.7082;\n",
      "Epoch: 120; train loss=0.7047;\n",
      "Epoch: 130; train loss=0.7011;\n",
      "Epoch: 140; train loss=0.6974;\n",
      "Epoch: 150; train loss=0.6936;\n",
      "Epoch: 160; train loss=0.6897;\n",
      "Epoch: 170; train loss=0.6857;\n",
      "Epoch: 180; train loss=0.6819;\n",
      "Epoch: 190; train loss=0.6783;\n",
      "Epoch: 199; train loss=0.6749;\n"
     ]
    }
   ],
   "source": [
    "x_all_train, y_all_train = load_train_dataset(trainCsvPath, trainImagesPath)\n",
    "x_all_train_tensor = torch.tensor(x_all_train, dtype=torch.float32)\n",
    "y_all_train_tensor = torch.tensor(y_all_train, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_losses_all = train_model(model, x_all_train_tensor, y_all_train_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12483c5f",
   "metadata": {},
   "source": [
    "# Load and predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08c36aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_dataset(model, csv_path, images_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    test_predictions_list = []\n",
    "    test_ids = []\n",
    "\n",
    "    print(\"Loading test images...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        img_filename = f\"{row['id_code']}.jpg\"\n",
    "        img_path = os.path.join(images_dir, img_filename)\n",
    "\n",
    "        try:\n",
    "            img_vector = load_and_preprocess_image(img_path)\n",
    "\n",
    "            img_tensor = torch.tensor(img_vector, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction = model(img_tensor)\n",
    "                pred_class = torch.argmax(prediction, dim=1).item()\n",
    "            \n",
    "            test_predictions_list.append(pred_class)\n",
    "            test_ids.append(row['id_code'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing image {img_path}: {e}\")\n",
    "\n",
    "            test_predictions_list.append(0)\n",
    "            test_ids.append(row['id_code'])\n",
    "\n",
    "    print(f\"Loaded and predicted {len(x)} sized test data\")\n",
    "\n",
    "    return test_ids, test_predictions_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51778a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\kan\\MultKAN.py:813: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1857.)\n",
      "  self.subnode_actscale.append(torch.std(x, dim=0).detach())\n",
      "c:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\kan\\MultKAN.py:823: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1857.)\n",
      "  input_range = torch.std(preacts, dim=0) + 0.1\n",
      "c:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\kan\\MultKAN.py:824: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1857.)\n",
      "  output_range_spline = torch.std(postacts_numerical, dim=0) # for training, only penalize the spline part\n",
      "c:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\kan\\MultKAN.py:825: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1857.)\n",
      "  output_range = torch.std(postacts, dim=0) # for visualization, include the contribution from both spline + symbolic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and predicted 3662 sized test data\n"
     ]
    }
   ],
   "source": [
    "test_ids, test_predictions = predict_test_dataset(\n",
    "    model, \n",
    "    testCsvPath, \n",
    "    testImagesPath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7dea38",
   "metadata": {},
   "source": [
    "# Create final file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f65cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(test_ids, predictions, output_path=\"res.csv\"):\n",
    "    df = pd.DataFrame({'id_code': test_ids, 'diagnosis': predictions})\n",
    "    \n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    unique, counts = np.unique(predictions, return_counts=True)\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        print(f\"  Class {cls}: {cnt} images ({cnt/len(predictions)*100:.1f}%)\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d61e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 0: 227 imgaes (11.8%)\n",
      "  Class 1: 67 imgaes (3.5%)\n",
      "  Class 2: 1502 imgaes (77.9%)\n",
      "  Class 3: 79 imgaes (4.1%)\n",
      "  Class 4: 53 imgaes (2.7%)\n",
      "\n",
      "First 10 rows:\n",
      "        id_code  diagnosis\n",
      "0  0005cfc8afb6          2\n",
      "1  003f0afdcd15          2\n",
      "2  006efc72b638          2\n",
      "3  00836aaacf06          2\n",
      "4  009245722fa4          2\n",
      "5  009c019a7309          2\n",
      "6  010d915e229a          1\n",
      "7  0111b949947e          0\n",
      "8  01499815e469          2\n",
      "9  0167076e7089          3\n"
     ]
    }
   ],
   "source": [
    "df = create_file(test_ids, test_predictions)\n",
    "\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4f12c",
   "metadata": {},
   "source": [
    "# Spline plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec381da3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "alpha (nan) is outside 0-1 range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\kan\\MultKAN.py:1199\u001b[39m, in \u001b[36mMultKAN.plot\u001b[39m\u001b[34m(self, folder, beta, metric, scale, tick, sample, in_vars, out_vars, title, varscale)\u001b[39m\n\u001b[32m   1196\u001b[39m                 color = \u001b[33m\"\u001b[39m\u001b[33mwhite\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1197\u001b[39m                 alpha_mask = \u001b[32m0.\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m             \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my0\u001b[49m\u001b[43m+\u001b[49m\u001b[43mz0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my0\u001b[49m\u001b[43m+\u001b[49m\u001b[43mz0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m/\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlw\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m             plt.plot([\u001b[32m1\u001b[39m / (\u001b[32m2\u001b[39m * N) + id_ / N, \u001b[32m1\u001b[39m / (\u001b[32m2\u001b[39m * n_next) + j / n_next], [l * (y0+z0) + y0/\u001b[32m2\u001b[39m + y1, l * (y0+z0)+y0], color=color, lw=\u001b[32m2\u001b[39m * scale, alpha=alpha[l][j][i] * alpha_mask)\n\u001b[32m   1203\u001b[39m \u001b[38;5;66;03m# plot connections (pre-mult to post-mult, post-mult = next-layer input)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\matplotlib\\pyplot.py:3838\u001b[39m, in \u001b[36mplot\u001b[39m\u001b[34m(scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   3830\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.plot)\n\u001b[32m   3831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m   3832\u001b[39m     *args: \u001b[38;5;28mfloat\u001b[39m | ArrayLike | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3836\u001b[39m     **kwargs,\n\u001b[32m   3837\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[32m-> \u001b[39m\u001b[32m3838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3843\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3844\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\matplotlib\\axes\\_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\matplotlib\\axes\\_base.py:546\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    544\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(result)\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\matplotlib\\axes\\_base.py:539\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    536\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlabel must be scalar or have the same length as the input \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    537\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(label)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_datasets\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m datasets.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m result = (\u001b[43mmake_artist\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mncx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mncy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m                      \u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m           \u001b[38;5;28;01mfor\u001b[39;00m j, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels))\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_kwargs:\n\u001b[32m    544\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\matplotlib\\axes\\_base.py:338\u001b[39m, in \u001b[36m_process_plot_var_args._make_line\u001b[39m\u001b[34m(self, axes, x, y, kw, kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m kw = {**kw, **kwargs}  \u001b[38;5;66;03m# Don't modify the original kw.\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;28mself\u001b[39m._setdefaults(\u001b[38;5;28mself\u001b[39m._getdefaults(kw), kw)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m seg = \u001b[43mmlines\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLine2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m seg, kw\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\matplotlib\\lines.py:421\u001b[39m, in \u001b[36mLine2D.__init__\u001b[39m\u001b[34m(self, xdata, ydata, linewidth, linestyle, color, gapcolor, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28mself\u001b[39m.set_markeredgewidth(markeredgewidth)\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# update kwargs before updating data to give the caller a\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# chance to init axes (and hence unit support)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_internal_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[38;5;28mself\u001b[39m.pickradius = pickradius\n\u001b[32m    423\u001b[39m \u001b[38;5;28mself\u001b[39m.ind_offset = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\matplotlib\\artist.py:1233\u001b[39m, in \u001b[36mArtist._internal_update\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_internal_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, kwargs):\n\u001b[32m   1227\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1228\u001b[39m \u001b[33;03m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[32m   1229\u001b[39m \u001b[33;03m    errors as if calling `set`.\u001b[39;00m\n\u001b[32m   1230\u001b[39m \n\u001b[32m   1231\u001b[39m \u001b[33;03m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_props\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{cls.__name__}\u001b[39;49;00m\u001b[33;43m.set() got an unexpected keyword argument \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{prop_name!r}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\matplotlib\\artist.py:1209\u001b[39m, in \u001b[36mArtist._update_props\u001b[39m\u001b[34m(self, props, errfmt)\u001b[39m\n\u001b[32m   1205\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[32m   1206\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1207\u001b[39m                     errfmt.format(\u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), prop_name=k),\n\u001b[32m   1208\u001b[39m                     name=k)\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m             ret.append(\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[32m   1211\u001b[39m     \u001b[38;5;28mself\u001b[39m.pchanged()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rost1\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\matplotlib\\artist.py:1026\u001b[39m, in \u001b[36mArtist.set_alpha\u001b[39m\u001b[34m(self, alpha)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1024\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33malpha must be numeric or None, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(alpha)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m0\u001b[39m <= alpha <= \u001b[32m1\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33malpha (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is outside 0-1 range\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m alpha != \u001b[38;5;28mself\u001b[39m._alpha:\n\u001b[32m   1028\u001b[39m     \u001b[38;5;28mself\u001b[39m._alpha = alpha\n",
      "\u001b[31mValueError\u001b[39m: alpha (nan) is outside 0-1 range"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
